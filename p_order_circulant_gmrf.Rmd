---
title: "p'th order circulant GMRF"
output: html_document
date: "2025-03-02"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rootSolve)
```


## Simulating from p'th order model
```{r}
circulant_matrix_base_p = function(kappa, betas, N) {
  order = length(betas)
  diag_entry = kappa
  remainder = c(kappa*betas, 
                rep(0, N-1-2*order), 
                rev(kappa*betas))
  return( c(diag_entry, remainder) )
}

create_circulant_precision_matrix_p = function(kappa, betas, N = 10) {
  base = circulant_matrix_base_p(kappa, betas, N)
  N = length(base)
  idx = outer(seq_len(N), seq_len(N), function(i,j) ((j-i) %% N) +1)  # Definition 2.4 Rue & Held
  matrix_out = matrix(base[idx], nrow = N)
  return (matrix_out)
}

simulate_circulant_GMRF = function(mu, precision_mat, nsim = 1) {
  Q = precision_mat
  N = nrow(Q)
  Sigma = solve(Q) 
  out = MASS::mvrnorm(n = nsim, mu = rep(mu, N), Sigma = Sigma)
  return(out)
}

# Q = create_circulant_precision_matrix_p(100, c(-0.2, -0.0), N = 100)
# V = simulate_circulant_GMRF(5, Q)

# plot(V, type = 'b', ylim = c(4.5,5.5))
```


Example to show condition on betas is sufficient, not necessary.
```{r}
N = 10
j_seq = 0:(N-1)
beta_1 = 0.3
beta_2 = 0.3
1 + 2*(beta_1*cos(2*pi*j_seq/N) + beta_2*cos(2*pi*2*j_seq/N))
```



## Inference

### Solving numerically, one realization
```{r}
set.seed(123)
N = 256
beta1_true = -0.2
beta2_true = -0.1
kappa_true = 2
mu_true = 0
Q = create_circulant_precision_matrix_p(kappa_true, c(beta1_true, beta2_true), N = N)
V = simulate_circulant_GMRF(mu_true, Q) %>% as.matrix %>% t

R_val <- function(v, mu, k) {
  v_ = v- mu
  shifted_indices <- ((0:(N-1) + k ) %% N) + 1
  return( sum(v_ * v_[shifted_indices]) ) 
}

R_val_vec <- Vectorize(R_val, 'k')

mu_est = mean(V)
sum_sq = sum((V-mu_est)^2)

kappa_hat <- function(beta_vec, mu) {
  p = length(beta_vec)
  R_vals = R_val_vec(V, mu, 1:p)
  denom = sum_sq + 2*sum(beta_vec*R_vals)
  return(N/denom)
}
```

```{r}
score_beta <- function(beta_vec, v, mu) {
  N <- length(v)
  p <- length(beta_vec)
  f <- numeric(p)
  
  kappa_est = kappa_hat(beta_vec, mu)
  
  for (k in 1:p) {
    
    j_seq = 0:(N-1)
    
    term1 <- sum(sapply(j_seq, function(j) {
      numerator = cos(2*pi*j*k/N)
      cosines_denom_j = cos(2*pi*j*(1:p)/N)
      denom = 1 + 2*sum(beta_vec*cosines_denom_j)
      
      return(numerator/denom)
    }))
    
    R_val_k = R_val(v, mu, k)
    term2 = kappa_est*R_val_k
    
    f[k] <- term1 - term2
  }
  return(f)
}
```

```{r}
library(rootSolve)

p <- 2
initial_guess <- c(-0.2, -0.1)
solution <- multiroot(f = function(bet) score_beta(bet, V, mu_est), start = initial_guess)
beta_est <- solution$root

kappa_final_est = kappa_hat(solution$root, mu_est)

c('kappa_hat'= kappa_final_est, 'mu_hat' = mu_est, 'beta_hat' = beta_est)
```

- Afhænger meget af startværdier


### Solving numerically with more than one realization

```{r}
R_val <- function(v, mu, k) {
  N <- length(v) # er det rigtigt?
  v_ = v- mu
  shifted_indices <- ((0:(N-1) + k ) %% N) + 1
  return( sum(v_ * v_[shifted_indices]) ) 
}

R_val_vec <- Vectorize(R_val, 'k')


kappa_hat_numerical <- function(beta_vec, mu, V) {
  m = nrow(V)
  N = ncol(V)
  p = length(beta_vec)
  sum_squares <- sum(apply(V, 1, function(x) sum((x-mu)^2)))
  R_vals = apply(V, 1, function(x) R_val_vec(x, mu, 1:p))
  denom = sum_squares + 2*sum(beta_vec*R_vals)
  return(N*m/denom)
}

score_beta_numerical <- function(beta_vec, V, mu) {
  N <- ncol(V)
  m <- nrow(V)
  p <- length(beta_vec)
  f <- numeric(p)
  
  kappa_est = kappa_hat_numerical(beta_vec, mu, V)
  
  for (k in 1:p) {
    
    j_seq = 0:(N-1)
    
    term1 <- m*sum(sapply(j_seq, function(j) {
      numerator = cos(2*pi*j*k/N)
      cosines_denom_j = cos(2*pi*j*(1:p)/N)
      denom = 1 + 2*sum(beta_vec*cosines_denom_j)
      
      return(numerator/denom)
    }))
    
    # R_val_k = R_val(v, mu, k)
    R_val_k = sum(apply(V, 1, function(x) R_val(x, mu, k)))
    term2 = kappa_est*R_val_k
    
    f[k] <- term1 - term2
  }
  return(f)
}
```


```{r}
#set.seed(123)
N = 100
beta1_true = -0.2
beta2_true = -0.1
kappa_true = 10
mu_true = 0
Q = create_circulant_precision_matrix_p(kappa_true, c(beta1_true, beta2_true), N = N)
V = simulate_circulant_GMRF(mu_true, Q, nsim = 250) 
```


```{r}
solve_params_numerical <- function(initial_guess_betas, V) {
  mu_est <- mean(V)
  solution <- multiroot(f = function(bet) score_beta_numerical(bet, V, mu_est), start = initial_guess_betas)
  beta_est_num <- solution$root
  kappa_est_num = kappa_hat_numerical(beta_est_num, mu_est, V)
  
  return(
    list('kappa_est' = kappa_est_num,
         'beta_est' = beta_est_num,
         'mu_est' = mu_est)
  )
}

# initial_guess_betas <- c(-0.05, -0.05)
# est_num <- solve_params_numerical(initial_guess_betas, V)
# est_num
```


### Solving betas analytically

```{r}
m = 250
N = 100
Q = create_circulant_precision_matrix_p(10, c(-0.3), N = N)
V_ = simulate_circulant_GMRF(0, Q, nsim = m) #%>% as.matrix %>% t
mu_est = mean(V_)
V = V_ #- mu_est 
```


```{r}
# A function to cyclically shift a vector by 'shift_by' positions to the left
cyclic_shift <- function(vec, shift_by) {
  n <- length(vec)
  if (shift_by == 0) {
    return(vec)
  }
  # shift by 1: [2:n, 1].  
  # shift by 2: [3:n, 1:2].
  return(c(vec[(shift_by+1):n], vec[1:shift_by]))
}


# Cycle-average covariance from ONE vector of length N
cov_cycle_one <- function(x) {
  N <- length(x)
  j_seq <- 0:(N-1)
  outer_prods <- lapply(j_seq, function(j) {
    x_shift <- cyclic_shift(x, j)
    return(outer(x_shift, x_shift))
  })
  return(Reduce("+", outer_prods) / N)
}


# Handle M realizations
cov_cycle_multi <- function(V) {
  # V is m x N
  m <- nrow(V)
  N <- ncol(V)
  
  cov_list <- vector("list", m)
  for (i in seq_len(m)) {
    x_i <- V[i, ]  # one realization
    cov_list[[i]] <- cov_cycle_one(x_i)
  }
  Sigma_est <- Reduce("+", cov_list) / m  
  return(Sigma_est)
}
```

```{r}
solve_params_analytical <- function(V, p) {
  Sigma_est <- cov_cycle_multi(V)
  Q_est <- solve(Sigma_est)
  kappa_est <- Q_est[1,1]  

  beta_est <- numeric(p)
  for (k in 1:p) {
    Q_0k <- Q_est[1, 1+k]
    beta_est[k] <- Q_0k / kappa_est
  }
  
  return(
    list('kappa_est' = kappa_est,
         'beta_est' = beta_est)
  )
}
# est_analytical <- solve_params_analytical(V, 2)
# est_analytical
```


### Histogram for many trials
```{r}
N_trials <- 1000

m_vals <- c(25, 100, 250)

par(mfrow = c(3,3))

N = 50
Q = create_circulant_precision_matrix_p(10, c(-0.3, -0.15), N = N)

for (m in m_vals) {

  res_kappa = numeric(N_trials)
  res_mu = numeric(N_trials)
  res_beta_1 = numeric(N_trials)
  res_beta_2 = numeric(N_trials)
  
  p <- 2
  
  for (t in 1:N_trials) {


    V = simulate_circulant_GMRF(0, Q, nsim = m) 
    mu_est = mean(V)
    
    Sigma_est <- cov_cycle_multi(V)
    Q_est <- solve(Sigma_est)
    
    kappa_est <- Q_est[1,1]  
  
    beta_est <- numeric(p)
    for (k in 1:p) {
      Q_0k <- Q_est[1, 1+k]
      beta_est[k] <- Q_0k / kappa_est
    }
    res_beta_1[t] <- beta_est[1]
    res_beta_2[t] <- beta_est[2]
    res_kappa[t] <- kappa_est
    res_mu[t] <- mu_est
  }
  # create x-axis limits from smallest m
  if(m == min(m_vals)) {
    kappa_min = min(res_kappa)
    kappa_max = max(res_kappa)
    beta1_min = min(res_beta_1)
    beta1_max = max(res_beta_1)
    beta2_min = min(res_beta_2)
    beta2_max = max(res_beta_2)
  }
  
  
  hist(res_kappa, breaks = 25, main = paste0('m = ', m), 
       xlab = latex2exp::TeX(r'($\kappa$)'),
       xlim = c(kappa_min, kappa_max))
  abline(v =10, lty = 'dashed', col = 'red', lwd = 1.5)
  
  hist(res_beta_1, breaks = 25, main = paste0('m = ', m), 
       xlab = latex2exp::TeX(r'($\beta_1$)'),
       xlim = c(beta1_min, beta1_max))
  abline(v = -0.3, lty = 'dashed', col = 'red', lwd = 1.5)
  
  hist(res_beta_2, breaks = 25, main = paste0('m = ', m), 
       xlab = latex2exp::TeX(r'($\beta_2$)'),
       xlim = c(beta2_min, beta2_max))
  abline(v = -0.15, lty = 'dashed', col = 'red', lwd = 1.5)
}

```

## Solving betas analytically - without all the permutations


```{r}
m = 250
N = 10
Q = create_circulant_precision_matrix_p(10, c(-0.3, -0.15), N = N)
V = simulate_circulant_GMRF(0, Q, nsim = m)
mu_est = mean(V)

cov_est_circulant <- function(V) {
  m <- dim(V)[1]
  N <- dim(V)[2]
  
  # circulant matrix base
  base = numeric(N)
  base[1] = 1/(m*N)*sum(apply(V, 1, function(x) sum(x^2)))
  
  for (k in 1:(N-1)) {
    matrix_entry = 1/(m*N)*sum(apply(V, 1, function(x) sum(x*cyclic_shift(x, k))))
    base[k+1] = matrix_entry
  }
  idx = outer(seq_len(N), seq_len(N), function(i,j) ((j-i) %% N) +1)  # Definition 2.4 Rue & Held
  matrix_out = matrix(base[idx], nrow = N)
}

near(cov_est_circulant(V), cov_cycle_multi(V)) %>% all
```


### plot of estimated betas in different order models
```{r}
set.seed(123)
N <- 50
nsim <- 100

Q1 <- create_circulant_precision_matrix_p(10, c(-0.25), N)
Q2 <- create_circulant_precision_matrix_p(10, c(-0.25, -0.2), N)
Q3 <- create_circulant_precision_matrix_p(10, c(-0.25, -0.1, -0.1), N)
Q4 <- create_circulant_precision_matrix_p(10, c(-0.1, -0.1, -0.1, -0.1), N)

mu_sim = 0
V1 = simulate_circulant_GMRF(mu_sim, Q1, nsim = nsim)
V2 = simulate_circulant_GMRF(mu_sim, Q2, nsim = nsim)
V3 = simulate_circulant_GMRF(mu_sim, Q3, nsim = nsim)
V4 = simulate_circulant_GMRF(mu_sim, Q4, nsim = nsim)

data = list('v1' = V1, 'v2' = V2, 'v3' = V3, 'v4' = V4)

p <- 8
matrix_out = matrix(nrow = 4, ncol = p+1)

for (i in 1:4) {
  V = data[[i]]
  Sigma_est = cov_cycle_multi(V)
  Q_est = solve(Sigma_est)
  
  kappa_est <- Q_est[1,1] 
  beta_est <- numeric(p)
  for (k in 1:p) {
    Q_0k <- Q_est[1, 1+k]
    beta_est[k] <- Q_0k / kappa_est
  }
  matrix_out[i, ] <- c(kappa_est, beta_est)
}
```


```{r}
df_estimates <- matrix_out %>% as.data.frame() %>% 
  rownames_to_column('p')
colnames(df_estimates) <- c('p', 'kappa', paste0('beta_', 1:8))

df_true <- tibble::tribble(
  ~p, ~kappa, ~beta_1, ~beta_2, ~beta_3, ~beta_4, ~beta_5, ~beta_6, ~beta_7, ~beta_8,
   1,  10,    -0.25,       0,       0,       0,     0,     0,     0,     0,
   2,  10,    -0.25,    -0.2,       0,       0,     0,     0,     0,     0,
   3,  10,    -0.25,    -0.1,    -0.1,       0,     0,     0,     0,     0,
   4,  10,    -0.1,     -0.1,    -0.1,    -0.1,     0,     0,     0,     0
)

df_true_long <- df_true %>%
  select(-kappa) %>% 
  pivot_longer(
    cols = -p,
    names_to = "parameter",
    values_to = "true_value"
  )
```

```{r}
df_estimates %>% select(-kappa) %>% 
  pivot_longer(-p, names_to = 'parameter', values_to = 'estimate') %>% 
  
  # Hvis vi sætter minus foran estimate, er det betingede korrelationer
  ggplot() +  
  geom_col(aes(x = parameter, y = estimate, color = 'Estimates'),width = 0.4) +
  geom_abline(slope = 0, intercept = 0.0, linetype = 'dashed', alpha = 0.4)+
  geom_point(
    data = df_true_long,
    aes(x = parameter, y = true_value, color = "True values"),
    shape = 95,  # underscore shape
    size = 4.5
  ) +
  facet_wrap(~p, labeller = label_bquote(p == .(p))) +
  theme_minimal() + 
  ylim(c(-0.28, 0.1)) + 
  labs(y = '', x= '') +
  scale_x_discrete(
    labels = function(x) {
      parse(text = paste0("beta[", gsub("beta_", "", x), "]"))
    }
  ) +
  scale_color_manual(
    name = NULL,
    values = c("True values" = "red", "Estimates" = "darkgrey")
  ) +
  theme(panel.spacing = unit(1.5, "lines"), 
        legend.position = c(0.5, 0.5),     
        legend.direction = "horizontal",
        legend.box = "horizontal",
    legend.justification = c(0.5, 0.5))
  

```




## Likelihood function in the p'th order model

log likelihood calculation
```{r}
# helper function
calc_quad_form <- function(v, kappa, beta) {
  N <- length(v)
  p <- length(beta)
  
  # diagonal
  quad <- kappa * sum(v^2)
  
  # Off diagonals
  for (k in 1:p) {
    shifted_indices <- ((0:(N-1) + k ) %% N) + 1
    inner_sum <- sum(v * v[shifted_indices])
    
    quad <- quad + 2 * kappa * beta[k] * inner_sum
  }
  
  return(quad)
}

# helper function
calc_log_det <- function(beta_vec, kappa, N) {
  p <- length(beta_vec)
  term1 = N*log(kappa)
  
  j_seq = 0:(N-1)
  term2 = sum(sapply(j_seq, function(j) {
    l_seq = 1:p
    cosines = cos(2*pi*l_seq*j/N)
    return(log(1 + 2*sum(beta_vec*cosines)))
  }))
  return(term1 + term2)
}

#log like, single realization
log_like_single <- function(beta_vec, kappa, mu, v) {
  N = length(v)
  log_det = calc_log_det(beta_vec, kappa, N)
  v_ = v -mu
  quad_form = calc_quad_form(v_, kappa, beta_vec)
  return(1/2*log_det - N/2*log(2*pi) - 1/2*quad_form)
}

# sum over m realizations
log_like <- function(beta_vec, kappa, mu, V) {
  apply(V, 1, function(x) {
    log_like_single(beta_vec, kappa, mu, x)
  }) %>% sum %>% return
}
```



## Profile likelihood - single

### Beta1

```{r}
beta1s = seq(-0.5, 0.5, by = 0.01)
n_vals = length(beta1s)

ll_beta1s = numeric(n_vals)
for (i in 1:n_vals) {
  beta1 = beta1s[i]
  ll_beta1s[i] <- log_like_single(c(beta1, beta2_true), kappa_true, mu_true, V)
}

plot(beta1s, ll_beta1s, type = 'l', xlab = latex2exp::TeX(r'($\beta_1$)'), ylab = 'log-likelihood')
abline(v = beta1_true, lty = 'dashed')
abline(v = beta_est[1], lty = 'dashed', col = 'red')
legend(x = 'topright', legend = c('True value', 'Estimate'), col = c('black', 'red'), lty = 'dashed', cex = 0.6)
```


### Beta2
```{r}
beta2s = seq(-0.5, 0.5, by = 0.01)
n_vals = length(beta1s)

ll_beta2s = numeric(n_vals)
for (i in 1:n_vals) {
  beta2 = beta2s[i]
  ll_beta2s[i] <- log_like_single(c(beta1_true, beta2), kappa_true, mu_true, V)
}

plot(beta2s, ll_beta2s, type = 'l', xlab = latex2exp::TeX(r'($\beta_2$)'), ylab = 'log-likelihood')
abline(v = beta2_true, lty = 'dashed')
abline(v = beta_est[2], lty = 'dashed', col = 'red')
legend(x = 'topright', legend = c('True value', 'Estimate'), col = c('black', 'red'), lty = 'dashed', cex = 0.6)
```


### Kappa
```{r}
kappas = seq(1, 4, by = 0.01)
n_vals = length(kappas)

ll_kappas = numeric(n_vals)
for (i in 1:n_vals) {
  kap = kappas[i]
  ll_kappas[i] <- log_like_single(c(beta1_true, beta2_true), kap, mu_true, V)
}

plot(kappas, ll_kappas, type = 'l', xlab = latex2exp::TeX(r'($\kappa$)'), ylab = 'log-likelihood')
abline(v = kappa_true, lty = 'dashed')
abline(v = kappa_final_est, lty = 'dashed', col = 'red')
legend(x = 'topright', legend = c('True value', 'Estimate'), col = c('black', 'red'), lty = 'dashed', cex = 0.6)
```

### mu
```{r}
mus = seq(4, 6, by = 0.01)
n_vals = length(mus)

ll_mus = numeric(n_vals)
for (i in 1:n_vals) {
  mu = mus[i]
  ll_mus[i] <- log_like_single(c(beta1_true, beta2_true), kappa_true, mu, V)
}

plot(mus, ll_mus, type = 'l', xlab = latex2exp::TeX(r'($\mu$)'), ylab = 'log-likelihood')
abline(v = mu_true, lty = 'dashed')
abline(v = mu_est, lty = 'dashed', col = 'red')
legend(x = 'topright', legend = c('True value', 'Estimate'), col = c('black', 'red'), lty = 'dashed', cex = 0.6)
```




## Profile likelihood - multiple


```{r}
set.seed(12345)
mu_true = 0
kappa_true = 10
beta_1_true = -0.25
beta_2_true = -0.1

m = 100
N = 50

Q = create_circulant_precision_matrix_p(kappa_true, c(beta_1_true, beta_2_true), N = N)
V = simulate_circulant_GMRF(mu_true, Q, nsim = m) #%>% as.matrix %>% t
mu_est = mean(V)

est_numerical = solve_params_numerical(c(-0.15, -0.15), V)
est_analytical = solve_params_analytical(V, 2)

```


### Kappa

```{r}
n_vals <- 100
kappas = seq(9, 11, length.out = n_vals)
ll_kappas = numeric(n_vals)
for (i in 1:n_vals) {
  kap = kappas[i]
  ll_kappas[i] <- log_like(c(est_numerical$beta_est[1], est_numerical$beta_est[2]), kap, mu_est, V)
}



plot(kappas, ll_kappas, type = 'l', xlab = latex2exp::TeX(r'($\kappa$)'), ylab = 'log-likelihood')
abline(v = kappa_true, lty = 'dashed')
abline(v = est_numerical$kappa_est, lty = 'dashed', col = 'red')
abline(v = est_analytical$kappa_est, lty = 'dashed', col = 'blue')
legend(x = 'topleft', legend = c('True value', 'Estimate numerical', 'Estimate analytical'), col = c('black', 'red', 'blue'), lty = 'dashed', cex = 0.6)
```

### Beta 1

```{r}
n_vals <- 100
beta_1s = seq(-0.27, -0.23, length.out = n_vals)
ll_beta_1s = numeric(n_vals)
for (i in 1:n_vals) {
  beta_1 = beta_1s[i]
  ll_beta_1s[i] <- log_like(c(beta_1,  est_numerical$beta_est[2]), est_numerical$kappa_est, mu_est, V)
}



plot(beta_1s, ll_beta_1s, type = 'l', xlab = latex2exp::TeX(r'($\beta_1$)'), ylab = 'log-likelihood')
abline(v = beta_1_true, lty = 'dashed')
abline(v = (est_numerical$beta_est)[1], lty = 'dashed', col = 'red')
abline(v = (est_analytical$beta_est)[1], lty = 'dashed', col = 'blue')
legend(x = 'topright', legend = c('True value', 'Estimate numerical', 'Estimate analytical'), col = c('black', 'red', 'blue'), lty = 'dashed', cex = 0.6)
```


### Beta 2

```{r}
n_vals <- 100
beta_2s = seq(-0.14, -0.06, length.out = n_vals)
ll_beta_2s = numeric(n_vals)
for (i in 1:n_vals) {
  beta_2 = beta_2s[i]
  ll_beta_2s[i] <- log_like(c( est_numerical$beta_est[1], beta_2),  est_numerical$kappa_est, mu_est, V)
}

plot(beta_2s, ll_beta_2s, type = 'l', xlab = latex2exp::TeX(r'($\beta_2$)'), ylab = 'log-likelihood')
abline(v = beta_2_true, lty = 'dashed')
abline(v = (est_numerical$beta_est)[2], lty = 'dashed', col = 'red')
abline(v = (est_analytical$beta_est)[2], lty = 'dashed', col = 'blue')
legend(x = 'topright', legend = c('True value', 'Estimate numerical', 'Estimate analytical'), col = c('black', 'red', 'blue'), lty = 'dashed', cex = 0.6)
```
